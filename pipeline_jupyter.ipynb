{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Run if you need to install libraries required\n",
        "\n",
        "\"\"\"\n",
        "pip install pandas\n",
        "pip install numpy\n",
        "pip install matplotlib\n",
        "pip install seaborn\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "etCRuogGuwBF",
        "outputId": "2533a323-edc3-4702-94e8-e05220e09f84"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npip install pandas\\npip install numpy\\npip install matplotlib\\npip install seaborn\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "phrXkX8FsxSn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd                 # For Data Manipulation\n",
        "import numpy as np                  # For Data Manipulation\n",
        "import matplotlib.pyplot as plt     # For Data Visualisation\n",
        "import seaborn as sns               # For Data Visualisation\n",
        "import sqlite3                      # For SQL\n",
        "import os                           # For Directory Management"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_popular_cars = [\"volkswagen\", \"ford\", \"bmw\", \"audi\", \"mercedes\",\n",
        "                        \"toyota\",\"nissan\", \"kia\", \"hyundai\", \"peugeot\"]\n"
      ],
      "metadata": {
        "id": "Tuci5CJ-zeSA"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loadTransform function loads the data, and is also able to convert specified columns to python string\n",
        "\n",
        "def loadTransform(category, object_columns=None):\n",
        "\n",
        "\n",
        "  try:\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__)) # File path where script is running\n",
        "    csv_path = os.path.join(script_dir, \"csv-last-5-years\", f\"dft-road-casualty-statistics-{category}-last-5-years.csv\")\n",
        "\n",
        "  except:\n",
        "    # __file__ does not exist in colab\n",
        "    csv_path = f\"./csv-last-5-years/dft-road-casualty-statistics-{category}-last-5-years.csv\"\n",
        "\n",
        "\n",
        "  df = pd.read_csv(csv_path)\n",
        "\n",
        "  # Converts columns specified in parameters to python string (intended for columns of mixed types)\n",
        "\n",
        "  if object_columns:\n",
        "      for column in object_columns:\n",
        "        if column == \"date\":\n",
        "          df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%d/%m/%Y\", errors=\"coerce\")\n",
        "        elif column == \"time\":\n",
        "          df[\"time\"] = pd.to_datetime(df[\"time\"], format=\"%H:%M\", errors=\"coerce\").dt.time\n",
        "        else:\n",
        "            df[column] = df[column].astype(str)\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "62jH-e2QtBCS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loadDescribe function: loads and describes the data\n",
        "\n",
        "def fullDescribe(df):\n",
        "\n",
        "\n",
        "  # (a) Dimension data\n",
        "\n",
        "  print(\"NUMBER OF ROWS AND COLUMN: \\n\")\n",
        "  print(df.shape) # Number of rows and columns\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # (b) Attribute types\n",
        "\n",
        "  print(\"COLUMNS AND DATA TYPES: \\n\");\n",
        "  print(df.info()) # Columns and data types\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # (c) Missing values\n",
        "\n",
        "  # Making a dataframe of the columns and missing value/percentage\n",
        "\n",
        "  print (\"DATAFRAME OF MISSING: \\n\")\n",
        "  df_replaceNA = df.replace([-1,\"-1\"],np.nan)\n",
        "  dfMissing = pd.DataFrame({\n",
        "    'Column Name': df_replaceNA.columns,\n",
        "    'Missing Count': df_replaceNA.isnull().sum(),\n",
        "    'Missing Percentage': (df_replaceNA.isnull().sum() / len(df_replaceNA) * 100).round(4)\n",
        "    })\n",
        "  print(dfMissing)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  # (d) Descriptive statistics\n",
        "\n",
        "  print(\"DESCRIPTIVE STATISTICS: \\n\")\n",
        "  print(df_replaceNA.describe(include='all')) # Descriptive Statistics\n",
        "  print(\"\\n\")\n",
        "\n",
        "  return\n",
        "\n"
      ],
      "metadata": {
        "id": "pHUh3ZB3vVSU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function used for mapping brand in visualisation and data cleaning\n",
        "\n",
        "def map_brand(model):\n",
        "  model = str(model).lower()\n",
        "  for brand in most_popular_cars:\n",
        "      if brand in model:\n",
        "          return brand.capitalize()\n",
        "  return np.nan"
      ],
      "metadata": {
        "id": "iwVyfWDXsKw5"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisations for Casualty\n",
        "\n",
        "def vizCasualty(df):\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  columns = [\"age_band_of_casualty\", \"casualty_severity\", \"casualty_distance_banding\"] # List of columns I will use for visualisations\n",
        "  df = df[columns].copy()\n",
        "\n",
        "  plt.figure(figsize=(12,6))\n",
        "\n",
        "  # Plot 1\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "\n",
        "   # Decoding the columns\n",
        "\n",
        "  age_labels = {-1:\"NA\", 1:\"0-5\",2:\"6-10\",3:\"11-15\",4:\"16-20\",5:\"21-25\", 6:\"26-35\",\n",
        "                7:\"36-45\",8:\"46-55\", 9:\"56-65\", 10:\"66-75\", 11:\"Over 75\"}\n",
        "\n",
        "\n",
        "  age_order = [\"0-5\",\"6-10\",\"11-15\",\"16-20\",\"21-25\",\"26-35\", \"36-45\",\"46-55\",\n",
        "               \"56-65\",\"66-75\",\"Over 75\"]\n",
        "\n",
        "  severity_labels = {1:\"Fatal\", 2:\"Serious\", 3:\"Slight\"}\n",
        "\n",
        "   # Adding decoded columns to DF (Mapping)\n",
        "\n",
        "  df[\"age_group\"] = df[\"age_band_of_casualty\"].map(age_labels)\n",
        "  df[\"age_group\"] = pd.Categorical(df[\"age_group\"], categories=age_order, ordered=True)\n",
        "  df[\"severity_label\"] = df[\"casualty_severity\"].map(severity_labels)\n",
        "\n",
        "  df_plot1 = df[df['age_group'] != \"NA\"].copy()\n",
        "\n",
        "  # Preparing for use in stacked bar\n",
        "\n",
        "  grouped = df_plot1.groupby(['age_group', \"severity_label\"], observed=False)\n",
        "  grouped_counts = grouped.size()\n",
        "  age_sev_count = grouped_counts.unstack()\n",
        "\n",
        "\n",
        "  age_sev_count.plot(\n",
        "      kind=\"bar\",\n",
        "      stacked=True,\n",
        "      colormap=\"Set1\", # Changing colour palette\n",
        "      ax=plt.gca() # Making sure it plots in this subplot\n",
        "    )\n",
        "\n",
        "\n",
        "  plt.title(\"Number of Casualties by Age Group\")\n",
        "  plt.xlabel(\"Age Group\")\n",
        "  plt.ylabel(\"Number of Casualties\")\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.legend(title=\"Severity\")\n",
        "\n",
        "\n",
        "  # Plot 2\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "\n",
        "  # Decoding the columns\n",
        "\n",
        "  distance_labels = {-1:\"NA\", 1:\"0-5\", 2:\"5-10\", 3:\"10-20\", 4:\"20-100\", 5:\"NA\"} # Made 100km+ NA so I can remove later\n",
        "  distance_order = [\"0-5\",\"5-10\",\"10-20\", \"20-100\"]\n",
        "  bin_width = {\"0-5\":5,\"5-10\":5,\"10-20\":10, \"20-100\":80}\n",
        "\n",
        "  # Adding decoded columns to DF (Mapping)\n",
        "\n",
        "  df[\"distance_group\"] = df[\"casualty_distance_banding\"].map(distance_labels)\n",
        "  df = df[df['distance_group'] != \"NA\"].copy()\n",
        "  df[\"distance_group\"] = pd.Categorical(df[\"distance_group\"], categories=distance_order, ordered=True)\n",
        "\n",
        "\n",
        "  # Adding frequency density\n",
        "\n",
        "  df = df.groupby([\"distance_group\", \"severity_label\"], observed=False).size().reset_index(name=\"count\")\n",
        "  df[\"distance_bin_width\"] = df[\"distance_group\"].map(bin_width)\n",
        "  df[\"log_freq_density\"] = np.log((df[\"count\"] / df[\"distance_bin_width\"]) +1)\n",
        "\n",
        "  df_pivot = df.pivot(index=\"distance_group\", columns=\"severity_label\", values=\"log_freq_density\").fillna(0)\n",
        "\n",
        "  # Plotting\n",
        "\n",
        "  sns.heatmap(df_pivot, cmap=\"YlGnBu\")\n",
        "  plt.title(\"Heatmap of Log(FD) by Distance & Severity\")\n",
        "  plt.xlabel(\"Severity Level\")\n",
        "  plt.ylabel(\"Distance from Home Postcode (km)\")\n",
        "  plt.yticks(rotation=0)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "lymltHXJxXTJ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisations for Collision\n",
        "\n",
        "def vizCollision(df):\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  columns = [\"local_authority_district\", \"day_of_week\", \"speed_limit\"]\n",
        "  df = df[columns].copy()\n",
        "\n",
        "  plt.figure(figsize=(12,6))\n",
        "\n",
        "  # Plot 1\n",
        "\n",
        "  # Decoding columns\n",
        "\n",
        "  london_boroughs = {\n",
        "    1: \"Westminster\", 2: \"Camden\", 3: \"Islington\", 4: \"Hackney\",\n",
        "    5: \"Tower Hamlets\", 6: \"Greenwich\", 7: \"Lewisham\",\n",
        "    8: \"Southwark\", 9: \"Lambeth\", 10: \"Wandsworth\", 11: \"Hammersmith and Fulham\",\n",
        "    12: \"Kensington and Chelsea\", 13: \"Waltham Forest\", 14: \"Redbridge\",\n",
        "    15: \"Havering\", 16: \"Barking and Dagenham\", 17: \"Newham\",\n",
        "    18: \"Bexley\", 19: \"Bromley\", 20: \"Croydon\", 21: \"Sutton\",\n",
        "    22: \"Merton\", 23: \"Kingston upon Thames\", 24: \"Richmond upon Thames\",\n",
        "    25: \"Hounslow\", 26: \"Hillingdon\", 27: \"Ealing\", 28: \"Brent\", 29: \"Harrow\",\n",
        "    30: \"Barnet\", 31: \"Haringey\", 32: \"Enfield\"}\n",
        "\n",
        "  inner_keys = list(range(1, 13))\n",
        "\n",
        "  df_plot1 = df[df[\"local_authority_district\"].isin(london_boroughs.keys())].copy() # Df of London\n",
        "  df_plot1[\"borough\"] = df_plot1[\"local_authority_district\"].map(london_boroughs)\n",
        "\n",
        "  df_plot1[\"region\"] = df_plot1[\"local_authority_district\"].apply(\n",
        "    lambda x: \"Inner London\" if x in inner_keys else \"Outer London\") # Mapping Region\n",
        "\n",
        "\n",
        "  day_labels = {1:\"Sunday\", 2:\"Monday\", 3:\"Tuesday\", 4:\"Wednesday\",\n",
        "                5:\"Thursday\", 6:\"Friday\", 7:\"Saturday\"}\n",
        "\n",
        "  day_order = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
        "\n",
        "\n",
        "  df_plot1[\"day_label\"] = df_plot1[\"day_of_week\"].map(day_labels)\n",
        "  df_plot1[\"day_label\"] = pd.Categorical(df_plot1[\"day_label\"], categories=day_order, ordered=True) # Mapping days (ordered)\n",
        "\n",
        "  # Preparing for graph\n",
        "\n",
        "  df_plot1 = df_plot1.groupby([\"region\", \"day_label\"],observed=False).size().reset_index(name=\"count\") # Observed = false to silence warnings about future updates (Just doing want the terminal was saying)\n",
        "\n",
        "  # Using proportions for clarity between regions\n",
        "\n",
        "  region_totals = df_plot1.groupby('region')['count'].transform('sum')\n",
        "  df_plot1['proportion'] = df_plot1['count'] / region_totals\n",
        "\n",
        "  df_plot1 = df_plot1.pivot(index=\"day_label\", columns=\"region\", values=\"proportion\")\n",
        "\n",
        "  reordered = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
        "  df_plot1 = df_plot1.reindex(reordered) # Reordering for graph\n",
        "\n",
        "  # Plotting\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "\n",
        "  df_plot1.plot(marker='o',\n",
        "                ax=plt.gca()\n",
        "  )\n",
        "  plt.legend(title=\"Region\")\n",
        "  plt.title(\"Proportion of Collisions in London per Day by Region\")\n",
        "  plt.xlabel(\"Day of Week\")\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.ylabel(\"Proportion\")\n",
        "\n",
        "  # Plot 2\n",
        "\n",
        "\n",
        "  df[\"speed_limit\"]= df[\"speed_limit\"].replace([-1,99],np.nan)\n",
        "  df = df.dropna()\n",
        "  df['speed_limit'] = df['speed_limit'].astype(int) # Because gets converted to float after NAs\n",
        "  df = df.groupby(\"speed_limit\").size().reset_index(name=\"count\")\n",
        "\n",
        "\n",
        "  # Plotting\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "  wedges, texts, autotexts = plt.pie(\n",
        "    df['count'],\n",
        "    autopct='%1.1f%%')\n",
        "\n",
        "\n",
        "  plt.title(\"Pie Chart of Collisions\")\n",
        "  plt.legend(wedges, df['speed_limit'], title=\"Speed Limit(Mph)\",\n",
        "             bbox_to_anchor=(1,1))\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "3B43LD5gybqu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisations for Vehicle\n",
        "\n",
        "def vizVehicle(df):\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  columns = [\"first_point_of_impact\", \"generic_make_model\", \"age_of_driver\"]\n",
        "  df = df[columns].copy()\n",
        "\n",
        "  plt.figure(figsize=(12,6))\n",
        "\n",
        "  # Plot 1\n",
        "\n",
        "  impact_labels =  {1: \"Front\",2: \"Back\",3: \"Offside\",4: \"Nearside\",}\n",
        "\n",
        "\n",
        "  # Mapping impact labels\n",
        "\n",
        "  df_plot1 = df[df[\"first_point_of_impact\"].isin(impact_labels.keys())].copy()\n",
        "  df_plot1[\"impact_point\"] = df_plot1[\"first_point_of_impact\"].map(impact_labels)\n",
        "  df_plot1 = df_plot1[\"impact_point\"].value_counts().reindex(impact_labels.values())\n",
        "\n",
        "  # Plotting\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "\n",
        "  df_plot1.plot(kind=\"bar\",\n",
        "      ax=plt.gca(),\n",
        "    )\n",
        "\n",
        "  plt.title(\"Bar Chart of Collisions by Impact Point\")\n",
        "  plt.xlabel(\"Impact Point\")\n",
        "  plt.ylabel(\"Number of Collisions\")\n",
        "  plt.xticks(rotation=90)\n",
        "\n",
        "  # Plot 2\n",
        "\n",
        "\n",
        "  df[\"age_of_driver\"]= df[\"age_of_driver\"].replace(-1,np.nan)\n",
        "  df = df.dropna()\n",
        "\n",
        "  df[\"brand\"] = df[\"generic_make_model\"].apply(map_brand) # Mapping car brand using created function\n",
        "\n",
        "  df = df.groupby(\"brand\").agg(\n",
        "    average_age = (\"age_of_driver\", \"mean\"),\n",
        "    count =(\"brand\", \"size\")).reset_index()\n",
        "\n",
        "  total_drivers = df['count'].sum()\n",
        "  df['proportion'] = df['count'] / total_drivers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "  sns.scatterplot(data=df,\n",
        "                  x=\"average_age\",\n",
        "                  y=\"proportion\",\n",
        "                  hue=\"brand\")\n",
        "\n",
        "  plt.title(\"Proportion of Collisions by Top 10 Car Brands in the UK\")\n",
        "  plt.legend(title=\"Most Driven in UK\")\n",
        "  plt.xlabel(\"Average age\")\n",
        "  plt.ylabel(\"Proportion of collisions\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "nSbfrb1lyb8E"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processCasualty(df):\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  # Calculating IQR\n",
        "\n",
        "  Q1 = df[\"age_of_casualty\"].quantile(0.25)\n",
        "  Q3 = df[\"age_of_casualty\"].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  lower = Q1 - (1.5 * IQR)\n",
        "  upper = Q3 + (1.5 * IQR)\n",
        "\n",
        "  df[\"age_of_casualty\"] = df[\"age_of_casualty\"].clip(lower, upper).astype(int) # Clip converts to float\n",
        "\n",
        "  df = df.replace([-1,\"-1\"], np.nan) # Replacing -1 to accurately count what data is missing\n",
        "\n",
        "  columns = df.columns[(df.isnull().sum() / len(df) * 100) < 3].tolist()\n",
        "\n",
        "  df = df.dropna(subset=columns) # Dropping missing values in that column\n",
        "\n",
        "  df[\"sex_of_casualty\"] = df[\"sex_of_casualty\"].replace(9,np.nan)\n",
        "  df = df.dropna(subset=[\"sex_of_casualty\"])\n",
        "\n",
        "  df[\"is_male\"] = (df[\"sex_of_casualty\"] == 1).astype(int) # Converts True to 1 else 0\n",
        "\n",
        "\n",
        "  # Missing values are code as -1, changing to na\n",
        "\n",
        "  df= df.replace([-1,\"-1\"], np.nan)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "mR1-ATVOEFXc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processCollision(df):\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  historic = [column for column in df.columns if \"historic\" in column] # Creates a list of columns that are historic\n",
        "  columns = historic + [\"enhanced_severity_collision\", \"collision_year\", \"local_authority_district\"]\n",
        "  df = df.drop(columns=columns)\n",
        "\n",
        "  df[\"casualty_per_vehicle\"] = df[\"number_of_casualties\"] / df[\"number_of_vehicles\"]\n",
        "\n",
        "  normal_conditions = (\n",
        "    (df[\"light_conditions\"] == 1) &           # Daylight\n",
        "    (df[\"weather_conditions\"] == 1) &         # Fine no high winds\n",
        "    (df[\"road_surface_conditions\"] == 1) &    # Dry\n",
        "    (df[\"special_conditions_at_site\"] == 0))  # No special conditions\n",
        "\n",
        "  df[\"normal_conditions\"] = normal_conditions.astype(int) # Normal conditions 1 (True) else 0 (False)\n",
        "\n",
        "\n",
        "  # Missing values are code as -1, changing to na\n",
        "\n",
        "  df= df.replace([-1,\"-1\"], np.nan)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "PaY8pO36Tls_"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processVehicle(df, most_popular_cars):\n",
        "\n",
        "  df = df.copy()\n",
        "\n",
        "  df[\"brand\"] = df[\"generic_make_model\"].apply(map_brand) # Mapping car brand using created function\n",
        "  most_popular_cars = [brand.capitalize() for brand in most_popular_cars] # List needs to be captialized to match accurately\n",
        "  df = df[df[\"brand\"].isin(most_popular_cars)].copy()\n",
        "\n",
        "  df[\"brand_code\"] = df[\"brand\"].astype(\"category\").cat.codes + 1 # Creating brand codes starting from 1\n",
        "\n",
        "  median = df[\"age_of_vehicle\"].median()\n",
        "  df[\"age_of_vehicle\"] = df[\"age_of_vehicle\"].replace(-1, median).fillna(median)\n",
        "\n",
        "  # Missing values are code as -1, changing to na\n",
        "\n",
        "  df= df.replace([-1,\"-1\"], np.nan)\n",
        "\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "jyxc1CBNtBXy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipelineTest1(dfCasualty, dfCollision, dfVehicle):\n",
        "\n",
        "  test_list = []\n",
        "\n",
        "  print(\"Test 1 Data Loading\")\n",
        "\n",
        "  # A Pass if the csv has the correct number of columns\n",
        "\n",
        "  if dfCasualty.shape[1] == 23:\n",
        "      print(\"PASS\")\n",
        "      test_list.append(\"PASS\")\n",
        "  else:\n",
        "      print(f\"FAIL: Expected 23 columns in Casualty, got {dfCasualty.shape[1]}\")\n",
        "      test_list.append(\"FAIL\")\n",
        "\n",
        "  if dfCollision.shape[1] == 44:\n",
        "      print(\"PASS\")\n",
        "      test_list.append(\"PASS\")\n",
        "  else:\n",
        "      print(f\"FAIL: Expected 44 columns in Collision, got {dfCollision.shape[1]}\")\n",
        "      test_list.append(\"FAIL\")\n",
        "\n",
        "  if dfVehicle.shape[1] == 32:\n",
        "      print(\"PASS\")\n",
        "      test_list.append(\"PASS\")\n",
        "  else:\n",
        "      print(f\"FAIL: Expected 32 columns in Vehicle, got {dfVehicle.shape[1]}\")\n",
        "      test_list.append(\"FAIL\")\n",
        "\n",
        "  print(\"Test 1 Complete\")\n",
        "\n",
        "  return test_list\n",
        "\n"
      ],
      "metadata": {
        "id": "YZwFo65ispRX"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipelineTest2(dfCasualtyProcessed, dfCollisionProcessed, dfVehicleProcessed, test_list = None):\n",
        "\n",
        "  if test_list is None:\n",
        "      test_list = []\n",
        "\n",
        "  print(\"Test 2 Data Transforming\")\n",
        "\n",
        "  # A Fail if any of the age columns are negative\n",
        "\n",
        "  if (dfCasualtyProcessed[\"age_of_casualty\"]<0).any():\n",
        "    print(\"FAIL: Negative age_of_casualty in Casualty\")\n",
        "    test_list.append(\"FAIL\")\n",
        "  else:\n",
        "    print(\"PASS\")\n",
        "    test_list.append(\"PASS\")\n",
        "\n",
        "  if (dfVehicleProcessed[\"age_of_driver\"]<0).any():\n",
        "    print(\"FAIL: Negative age_of_driver in Vehicle\")\n",
        "    test_list.append(\"FAIL\")\n",
        "  else:\n",
        "    print(\"PASS\")\n",
        "    test_list.append(\"PASS\")\n",
        "\n",
        "  if(dfVehicleProcessed[\"age_of_vehicle\"]<0).any():\n",
        "    print(\"FAIL: Negative age_of_vehicle in Vehicle\")\n",
        "    test_list.append(\"FAIL\")\n",
        "  else:\n",
        "    print(\"PASS\")\n",
        "    test_list.append(\"PASS\")\n",
        "\n",
        "  print(\"Test 2 Complete\")\n",
        "\n",
        "\n",
        "  return test_list\n",
        "\n"
      ],
      "metadata": {
        "id": "H4IAzBZVzxD4"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipelineTest3(test_list = None):\n",
        "\n",
        "  if test_list is None:\n",
        "    test_list = []\n",
        "\n",
        "  print(\"Test 3 Data Serving\")\n",
        "\n",
        "  # A Pass if each table has at least one row\n",
        "\n",
        "  db_name = \"uk_road_safety_data.db\"\n",
        "\n",
        "  # Making directory is where script is running\n",
        "\n",
        "  try:\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "  except:\n",
        "    script_dir = \".\"\n",
        "\n",
        "  db_path = os.path.join(script_dir, db_name)\n",
        "\n",
        "  conn = sqlite3.connect(db_path)\n",
        "  cursor = conn.cursor()\n",
        "\n",
        "\n",
        "  tables = [\"fact_vehicle\", \"dim_collision\", \"dim_vehicle\", \"dim_datetime\"] # Each table name in db\n",
        "\n",
        "  for table in tables:\n",
        "    cursor.execute(f\"SELECT 1 FROM {table} LIMIT 1;\")  # Check if theres any data\n",
        "    result = cursor.fetchone() # One row of Data\n",
        "    if result is None:\n",
        "\n",
        "      print(f\"FAIL: {table} has no data\")\n",
        "      test_list.append(\"FAIL\")\n",
        "\n",
        "    else:\n",
        "      print(\"PASS\")\n",
        "      test_list.append(\"PASS\")\n",
        "\n",
        "  print(\"Test 3 Complete\")\n",
        "\n",
        "  conn.close()\n",
        "\n",
        "  return test_list"
      ],
      "metadata": {
        "id": "agRlsRtcBhHM"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling functions thats loads csv, describes dataset, creates visualisations and processes dataset for Casualty\n",
        "\n",
        "object_columns = [\"collision_index\", \"collision_ref_no\", \"lsoa_of_casualty\"]\n",
        "dfCasualty = loadTransform(\"casualty\", object_columns) # Function for loading data\n",
        "\n",
        "#fullDescribe(dfCasualty)  # Function describes dataset\n",
        "#vizCasualty(dfCasualty)   # Function creates visualisations\n",
        "\n",
        "dfCasualtyProcessed = processCasualty(dfCasualty) # Function processes data"
      ],
      "metadata": {
        "id": "RkMk3vRywdVC"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling functions thats loads csv, describes dataset, creates visualisations and processes dataset for Collision\n",
        "\n",
        "object_columns = [\"collision_index\", \"collision_ref_no\", \"date\", \"time\",\n",
        "                  \"local_authority_ons_district\", \"local_authority_highway\",\n",
        "                  \"local_authority_highway_current\",\"lsoa_of_accident_location\"]\n",
        "\n",
        "dfCollision = loadTransform(\"collision\",object_columns)\n",
        "\n",
        "#fullDescribe(dfCollision)\n",
        "#vizCollision(dfCollision)\n",
        "\n",
        "dfCollisionProcessed = processCollision(dfCollision)"
      ],
      "metadata": {
        "id": "pKaloVb6kSjl"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling functions thats loads csv, describes dataset, creates visualisations and processes dataset for Vehicle\n",
        "\n",
        "object_columns = [\"collision_index\",\"collision_ref_no\",\"generic_make_model\",\"lsoa_of_driver\"]\n",
        "\n",
        "dfVehicle = loadTransform(\"vehicle\", object_columns)\n",
        "\n",
        "#fullDescribe(dfVehicle)\n",
        "#vizVehicle(dfVehicle)\n",
        "\n",
        "dfVehicleProcessed = processVehicle(dfVehicle, most_popular_cars)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8FpXgElkaVJ",
        "outputId": "4c5f2554-375d-48a5-b570-34ba0b62bfa4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1007946677.py:15: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating tables in sql using sqlite\n",
        "\n",
        "db_name = \"uk_road_safety_data.db\"\n",
        "\n",
        "# Making sure db file is created in script directory\n",
        "\n",
        "try:\n",
        "  script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "except: # __file__ doesnt exist in colab\n",
        "  script_dir = \".\"\n",
        "\n",
        "db_path = os.path.join(script_dir, db_name)\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "\n",
        "# dim_casualty  - left in to show understanding of where I went wrong but is not created reflected in ERD\n",
        "\n",
        "create_dim_casualty = '''\n",
        "CREATE TABLE IF NOT EXISTS dim_casualty(\n",
        "\n",
        "    collision_index TEXT,\n",
        "    collision_year INT,\n",
        "    collision_ref_no TEXT,\n",
        "    vehicle_reference INT,\n",
        "    casualty_reference INT,\n",
        "    casualty_class INT,\n",
        "    sex_of_casualty INT,\n",
        "    is_male INT,\n",
        "    age_of_casualty INT,\n",
        "    age_band_of_casualty TEXT,\n",
        "    casualty_severity INT,\n",
        "    pedestrian_location INT,\n",
        "    pedestrian_movement INT,\n",
        "    car_passenger INT,\n",
        "    bus_or_coach_passenger INT,\n",
        "    pedestrian_road_maintenance_worker INT,\n",
        "    casualty_type INT,\n",
        "    casualty_imd_decile INT,\n",
        "    lsoa_of_casualty TEXT,\n",
        "    enhanced_casualty_severity INT,\n",
        "    casualty_injury_based INT,\n",
        "    casualty_adjusted_severity_serious FLOAT,\n",
        "    casualty_adjusted_severity_slight FLOAT,\n",
        "    casualty_distance_banding INT,\n",
        "    PRIMARY KEY(collision_index, vehicle_reference, casualty_reference)\n",
        "\n",
        ");\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# dim_collision\n",
        "\n",
        "create_dim_collision = '''\n",
        "CREATE TABLE IF NOT EXISTS dim_collision(\n",
        "\n",
        "    collision_index TEXT PRIMARY KEY,\n",
        "    collision_ref_no TEXT,\n",
        "    location_easting_osgr FLOAT,\n",
        "    location_northing_osgr FLOAT,\n",
        "    longitude FLOAT,\n",
        "    latitude FLOAT,\n",
        "    police_force INT,\n",
        "    collision_severity INT,\n",
        "    number_of_vehicles INT,\n",
        "    number_of_casualties INT,\n",
        "    date TEXT,\n",
        "    day_of_week INT,\n",
        "    time TEXT,\n",
        "    local_authority_ons_district TEXT,\n",
        "    local_authority_highway TEXT,\n",
        "    local_authority_highway_current TEXT,\n",
        "    first_road_class INT,\n",
        "    first_road_number INT,\n",
        "    road_type INT,\n",
        "    speed_limit INT,\n",
        "    junction_detail INT,\n",
        "    junction_control INT,\n",
        "    second_road_class INT,\n",
        "    second_road_number INT,\n",
        "    pedestrian_crossing INT,\n",
        "    light_conditions INT,\n",
        "    weather_conditions INT,\n",
        "    road_surface_conditions INT,\n",
        "    special_conditions_at_site INT,\n",
        "    carriageway_hazards INT,\n",
        "    urban_or_rural_area INT,\n",
        "    did_police_officer_attend_scene_of_accident INT,\n",
        "    trunk_road_flag INT,\n",
        "    lsoa_of_accident_location TEXT,\n",
        "    collision_injury_based INT,\n",
        "    collision_adjusted_severity_serious FLOAT,\n",
        "    collision_adjusted_severity_slight FLOAT,\n",
        "    casualty_per_vehicle FLOAT,\n",
        "    normal_conditions INT\n",
        "\n",
        ");\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# dim_vehicle\n",
        "\n",
        "create_dim_vehicle = '''\n",
        "CREATE TABLE IF NOT EXISTS dim_vehicle(\n",
        "\n",
        "    collision_index TEXT,\n",
        "    vehicle_reference INT,\n",
        "    collision_year INT,\n",
        "    collision_ref_no TEXT,\n",
        "    vehicle_type INT,\n",
        "    towing_and_articulation INT,\n",
        "    vehicle_manoeuvre_historic INT,\n",
        "    vehicle_manoeuvre INT,\n",
        "    vehicle_direction_from INT,\n",
        "    vehicle_direction_to INT,\n",
        "    vehicle_location_restricted_lane_historic INT,\n",
        "    vehicle_location_restricted_lane INT,\n",
        "    junction_location INT,\n",
        "    skidding_and_overturning INT,\n",
        "    hit_object_in_carriageway INT,\n",
        "    vehicle_leaving_carriageway INT,\n",
        "    hit_object_off_carriageway INT,\n",
        "    first_point_of_impact INT,\n",
        "    vehicle_left_hand_drive INT,\n",
        "    journey_purpose_of_driver_historic INT,\n",
        "    journey_purpose_of_driver INT,\n",
        "    sex_of_driver INT,\n",
        "    age_of_driver INT,\n",
        "    age_band_of_driver INT,\n",
        "    engine_capacity_cc INT,\n",
        "    propulsion_code INT,\n",
        "    age_of_vehicle INT,\n",
        "    generic_make_model TEXT,\n",
        "    driver_imd_decile INT,\n",
        "    lsoa_of_driver TEXT,\n",
        "    escooter_flag INT,\n",
        "    driver_distance_banding INT,\n",
        "    brand TEXT,\n",
        "    brand_code INT,\n",
        "    PRIMARY KEY (collision_index, vehicle_reference));\n",
        "\n",
        " '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# dim_datetime\n",
        "\n",
        "# Extracting the date and time and day of week from the collision table\n",
        "\n",
        "dim_datetime = pd.DataFrame()\n",
        "dim_datetime['collision_date'] = dfCollisionProcessed['date']\n",
        "dim_datetime['collision_time'] = dfCollisionProcessed['time']\n",
        "dim_datetime['year'] = dfCollisionProcessed['date'].apply(lambda x: x.year)\n",
        "dim_datetime['month'] = dfCollisionProcessed['date'].apply(lambda x: x.month)\n",
        "dim_datetime['day'] = dfCollisionProcessed['date'].apply(lambda x: x.day)\n",
        "dim_datetime['day_of_week'] = dfCollisionProcessed['day_of_week']\n",
        "dim_datetime['hour'] = dfCollisionProcessed['time'].apply(lambda x: x.hour)\n",
        "dim_datetime['minute'] = dfCollisionProcessed['time'].apply(lambda x: x.minute)\n",
        "\n",
        "\n",
        "create_dim_datetime = '''\n",
        "CREATE TABLE IF NOT EXISTS dim_datetime(\n",
        "\n",
        "    datetime_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    collision_date TEXT,\n",
        "    collision_time TEXT,\n",
        "    year INT,\n",
        "    month INT,\n",
        "    day INT,\n",
        "    hour INT,\n",
        "    minute INT,\n",
        "    day_of_week INT);\n",
        "\n",
        " '''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " # fact_vehicle\n",
        "\n",
        "create_fact_vehicle = '''\n",
        "CREATE TABLE IF NOT EXISTS fact_vehicle(\n",
        "    collision_index TEXT,\n",
        "    vehicle_reference INT,\n",
        "    datetime_id INT,\n",
        "\n",
        "    PRIMARY KEY (collision_index, vehicle_reference),\n",
        "    FOREIGN KEY (collision_index) REFERENCES dim_collision(collision_index),\n",
        "    FOREIGN KEY (collision_index, vehicle_reference) REFERENCES dim_vehicle(collision_index, vehicle_reference),\n",
        "    FOREIGN KEY (datetime_id) REFERENCES dim_datetime(datetime_id)\n",
        ");\n",
        "\n",
        "'''\n",
        "\n",
        "# Executing and committing\n",
        "\n",
        "cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
        "#cursor.execute(create_dim_casualty) # Commented out to reflect change in ERD\n",
        "cursor.execute(create_dim_collision)\n",
        "cursor.execute(create_dim_vehicle)\n",
        "cursor.execute(create_dim_datetime)\n",
        "cursor.execute(create_fact_vehicle)\n",
        "conn.commit()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V4GjbJUnfN1r"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data into database\n",
        "\n",
        "# dfCasualtyProcessed.to_sql('dim_casualty', conn, if_exists='append', index=False) # Commented out to reflect change in ERD\n",
        "dfCollisionProcessed.to_sql('dim_collision', conn, if_exists='append', index=False)\n",
        "dfVehicleProcessed.to_sql('dim_vehicle', conn, if_exists='append', index=False)\n",
        "dim_datetime.to_sql('dim_datetime', conn, if_exists='append', index=False)\n",
        "\n",
        "# Joining datasets to create fact_vehicle dataframe\n",
        "\n",
        "\"\"\"\n",
        "# Previous implementation of Casualty\n",
        "\n",
        "fact_vehicle = dfVehicleProcessed.merge(\n",
        "    dfCasualtyProcessed,\n",
        "    on=['collision_index', 'vehicle_reference'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Merging Collision\n",
        "\n",
        "fact_vehicle = dfVehicleProcessed.merge(\n",
        "    dfCollisionProcessed[['collision_index', 'date', 'time']],\n",
        "    on='collision_index',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merging datetime\n",
        "\n",
        "dim_datetime_db = pd.read_sql_query(\"SELECT * FROM dim_datetime\", conn) # Need to get autoincremented datetime_id\n",
        "\n",
        "\n",
        "# Ensure type consistency for merging date/time\n",
        "\n",
        "fact_vehicle['date_str'] = pd.to_datetime(fact_vehicle['date']).dt.date.astype(str)\n",
        "fact_vehicle['time_str'] = pd.to_datetime(fact_vehicle['time'], format='%H:%M:%S', errors='coerce').dt.time.astype(str)\n",
        "\n",
        "dim_datetime_db['collision_date_str'] = pd.to_datetime(dim_datetime_db['collision_date']).dt.date.astype(str)\n",
        "dim_datetime_db['collision_time_str'] = pd.to_datetime(dim_datetime_db['collision_time']).dt.time.astype(str)\n",
        "\n",
        "# Merging to get datetime_id (changed to inner to ensure datetime_id is not null)\n",
        "fact_vehicle = fact_vehicle.merge(\n",
        "    dim_datetime_db[['datetime_id', 'collision_date_str', 'collision_time_str']],\n",
        "    left_on=['date_str', 'time_str'],\n",
        "    right_on=['collision_date_str', 'collision_time_str'],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Keeping columns needed for fact table\n",
        "\n",
        "fact_vehicle = fact_vehicle[['collision_index', 'vehicle_reference', 'datetime_id']]\n",
        "fact_vehicle = fact_vehicle.drop_duplicates(subset=['collision_index', 'vehicle_reference']) # Duplicates exist from merging\n",
        "fact_vehicle.to_sql('fact_vehicle', conn, if_exists='append', index=False)\n",
        "\n",
        "\n",
        "conn.close() # Close connection after all operations are done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV27qJXe78Iy",
        "outputId": "91a3d02a-6515-4857-c484-a7d3e9f76930"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4188191098.py:40: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  dim_datetime_db['collision_time_str'] = pd.to_datetime(dim_datetime_db['collision_time']).dt.time.astype(str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Testing\n",
        "\n",
        "# Each test function prints result of each test but also returns a list\n",
        "\n",
        "test_list = pipelineTest1(dfCasualty,dfCollision,dfVehicle) # Function for testing csv loading\n",
        "test_list = pipelineTest2(dfCasualtyProcessed,dfCollisionProcessed,dfVehicleProcessed, test_list) # Function for testing data transforming\n",
        "test_list = pipelineTest3(test_list) # Function for testing db file\n",
        "print(\"\\nFull Test Results: \")\n",
        "print(test_list)\n",
        "\n",
        "\n",
        "if \"FAIL\" in test_list:\n",
        "    print(\"One or more tests failed\")\n",
        "else:\n",
        "    print(\"All Pipeline Tests Passed\")\n"
      ],
      "metadata": {
        "id": "8jwdH9Xj6UHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa6b642-ad73-41ea-a2cb-65162cd2a09d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1 Data Loading\n",
            "PASS\n",
            "PASS\n",
            "PASS\n",
            "Test 1 Complete\n",
            "Test 2 Data Transforming\n",
            "PASS\n",
            "PASS\n",
            "PASS\n",
            "Test 2 Complete\n",
            "Test 3 Data Serving\n",
            "PASS\n",
            "PASS\n",
            "PASS\n",
            "PASS\n",
            "Test 3 Complete\n",
            "\n",
            "Full Test Results: \n",
            "['PASS', 'PASS', 'PASS', 'PASS', 'PASS', 'PASS', 'PASS', 'PASS', 'PASS', 'PASS']\n",
            "All Pipeline Tests Passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1TZyNxR9DTj"
      },
      "execution_count": 79,
      "outputs": []
    }
  ]
}